# -*- coding: utf-8 -*-
"""challenge3-ARIMA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mSxpG1IYvuNSoJrQLwTFnuMQdogrcvsx
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import product

from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

import statsmodels.api as sm

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt
from statsmodels.tsa.arima.model import ARIMA
from pandas.plotting import autocorrelation_plot

from statsmodels.tsa.stattools import adfuller

import warnings
warnings.filterwarnings('ignore')

#Read CSV files
df_customer = pd.read_csv(r'/content/drive/MyDrive/PBI Kalbe Data Science/Case Study/customer.csv', delimiter=';')
df_product = pd.read_csv(r'/content/drive/MyDrive/PBI Kalbe Data Science/Case Study/product.csv', delimiter=';')
df_store = pd.read_csv(r'/content/drive/MyDrive/PBI Kalbe Data Science/Case Study/store.csv', delimiter=';')
df_transaction = pd.read_csv(r'/content/drive/MyDrive/PBI Kalbe Data Science/Case Study/transaction.csv', delimiter=';')

def check_and_clean_data(df):
    # Initial counts of duplicates and missing data
    initial_duplicates = df.duplicated().sum()
    initial_missing = df.isnull().sum().sum()

    # Print initial counts
    print("Initial Count of Duplicates:", initial_duplicates)
    print("Initial Count of Missing Data:", initial_missing)

    # Dropping duplicates and missing values
    df_cleaned = df.drop_duplicates()
    df_cleaned = df_cleaned.dropna()

    # Final counts of duplicates and missing data
    final_duplicates = df_cleaned.duplicated().sum()
    final_missing = df_cleaned.isnull().sum().sum()

    # Print final counts
    print("Final Count of Duplicates:", final_duplicates)
    print("Final Count of Missing Data:", final_missing)

    return df_cleaned

#Data cleaning for customer csv file
df_customer.head()

# Data cleansing for df_customer by replacing ',' with '.' for the 'Income' column
df_customer['Income'] = df_customer['Income'].replace('[,]', '.', regex=True).astype('float')

df_customer = check_and_clean_data(df_customer)

#Data cleaning for product csv file

df_product.head()

df_product = check_and_clean_data(df_product)

#Data cleaning for store csv file

df_store.head()

# Data cleansing for df_store by replacing ',' with '.'
df_store['Latitude'] = df_store['Latitude'].replace('[,]', '.', regex=True).astype('float')
df_store['Longitude'] = df_store['Longitude'].replace('[,]', '.', regex=True).astype('float')

df_store = check_and_clean_data(df_store)

#Data cleaning for transaction csv file

df_transaction.head()

df_transaction = check_and_clean_data(df_transaction)

# Data cleansing for df_transaction by changing the Date format to datetime
df_transaction['Date'] = pd.to_datetime(df_transaction['Date'])

df_transaction.head()

print(df_transaction['TransactionID'].value_counts())

print(df_transaction[df_transaction['TransactionID'] == 'TR71313'])
print(df_transaction[df_transaction['TransactionID'] == 'TR42197'])

"""From the result above, shown that some TransactionIDs are used for different CustomersID on different dates. This might indicate a data input error or inconsistency."""

# Group by 'TransactionID' and select the row with the maximum 'Date'
df_transaction = df_transaction.sort_values(by='Date', ascending=False) \
    .groupby('TransactionID', as_index=False).first()

print(df_transaction[df_transaction['TransactionID'] == 'TR71313'])
print(df_transaction[df_transaction['TransactionID'] == 'TR42197'])

#merge all data

# Merge the DataFrames
df_merge = pd.merge(df_transaction, df_customer, on='CustomerID', how='inner')
df_merge = pd.merge(df_merge, df_product.drop(columns=('Price')), on='ProductID', how='inner')
df_merge = pd.merge(df_merge, df_store, on='StoreID', how='inner')

df_merge.head()

#machine learning: regression

df_regresi = df_merge.groupby(['Date']).agg({
    'Qty' : 'sum'
}).reset_index()

df_regresi

decomposed = seasonal_decompose(df_regresi.set_index('Date'))

plt.figure(figsize=(8, 8))
plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend')
plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonality')
plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residuals')
plt.tight_layout()

"""### **Check Stationary Data**"""

cut_off = round(df_regresi.shape[0]*0.8)
df_train = df_regresi[:cut_off]
df_test = df_regresi[cut_off:].reset_index(drop=True)
df_train.shape, df_train.shape

df_train

df_test

plt.figure(figsize=(20,5))
sns.lineplot(data=df_train, x=df_train['Date'], y=df_train['Qty']);
sns.lineplot(data=df_test, x=df_test['Date'], y=df_test['Qty']);

from statsmodels.tsa.stattools import adfuller
result = adfuller(df_regresi['Qty'])
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print('\t%s: %.3f' % (key, value))

"""Based on the Dickey-Fuller test,the data is **stationary** since p-value < 0.05

### **Choosing p, d, and q value**
"""

autocorrelation_plot(df_regresi['Qty']);

"""#### **Choosing p, d, and q values**"""

import itertools

p=range(0,20)
q=range(0,10)
d=range(0,2)

pdq_combinations=list(itertools.product(p,d,q))
print(len(pdq_combinations))
print(pdq_combinations)

cut_off = round(df_regresi.shape[0]*0.8)
train = df_regresi[:cut_off]
test = df_regresi[cut_off:].reset_index(drop=True)
train.shape, train.shape

rmse=[]
order1=[]

train = train.set_index('Date')
test = test.set_index('Date')
y = train['Qty']

for pdq in pdq_combinations:
  try:
    model=ARIMA(train, order=pdq).fit()
    pred=model.predict(start=len(train), end=(len(df_regresi)-1))
    error=np.sqrt(mean_squared_error(test,pred))
    order1.append(pdq)
    rmse.append(error)

  except:
    continue

results=pd.DataFrame(index=order1, data=rmse, columns=['RMSE'])

results.head()

sorted_results = results.sort_values(by='RMSE', ascending=True)
sorted_results

file_path = '/content/drive/MyDrive/PBI Kalbe Data Science/pdq_values2.csv'
sorted_results.to_csv(file_path, index=False)

"""### **ARIMA**"""

def calculate_rmse(y_actual, y_pred):
    '''
    Function to calculate RMSE
    '''
    return mean_squared_error(y_actual, y_pred) ** 0.5

def evaluate_model(y_actual, y_pred):
    '''
    Function to evaluate machine learning model
    '''
    rmse = calculate_rmse(y_actual, y_pred)
    mae = mean_absolute_error(y_actual, y_pred)
    return rmse, mae

# Your existing code
df_train = df_train.set_index('Date')
df_test = df_test.set_index('Date')
y = df_train['Qty']

ARIMAmodel = ARIMA(y, order=(9, 1, 6)).fit()

y_pred = ARIMAmodel.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = ARIMAmodel.predict(start=y_pred_df.index[0], end=y_pred_df.index[-1])
y_pred_df.index = df_test.index
y_pred_out = y_pred_df['predictions']

rmse_value, mae_value = evaluate_model(df_test['Qty'], y_pred_out)

plt.figure(figsize=(20, 5))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color='red')
plt.plot(y_pred_out, color='black', label='ARIMA Predictions')
plt.legend()

print(f'RMSE value: {rmse_value}')
print(f'MAE value: {mae_value}')

y_pred_out

"""### **Predict Future Data**"""

#Build model on full dataset
y_reg = df_regresi.set_index('Date')
y_reg = y_reg['Qty']
final_model = ARIMA(y_reg, order=(9, 1, 6)).fit()
#Forecast for the next 30 days
prediction = final_model.predict(len(y_reg), len(y_reg)+30)

prediction

plt.figure(figsize=(20, 5))
plt.plot(y_reg, label='Train')
plt.plot(prediction, color='black', label='30 Days Predictions')
plt.legend()