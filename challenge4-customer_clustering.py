# -*- coding: utf-8 -*-
"""Kalbe Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jcjvZ_0p6aG5T9caJG7ElRZIcrp42viu
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import product

from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

import statsmodels.api as sm

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt
from statsmodels.tsa.arima.model import ARIMA
from pandas.plotting import autocorrelation_plot

import warnings
warnings.filterwarnings('ignore')

df_customer = pd.read_csv(r'/content/drive/MyDrive/PBI Kalbe Data Science/Case Study/Case Study - Customer.csv', delimiter=';')
df_product = pd.read_csv(r'/content/drive/MyDrive/PBI Kalbe Data Science/Case Study/Case Study - Product.csv', delimiter=';')
df_store = pd.read_csv(r'/content/drive/MyDrive/PBI Kalbe Data Science/Case Study/Case Study - Store.csv', delimiter=';')
df_transaction = pd.read_csv(r'/content/drive/MyDrive/PBI Kalbe Data Science/Case Study/Case Study - Transaction.csv', delimiter=';')

df_customer.dropna(subset=['Marital Status'], inplace=True)
# Data cleansing for df_customer by replacing ',' with '.' for the 'Income' column
df_customer['Income'] = df_customer['Income'].replace('[,]', '.', regex=True).astype('float')
# Data cleansing for df_store by replacing ',' with '.'
df_store['Latitude'] = df_store['Latitude'].replace('[,]', '.', regex=True).astype('float')
df_store['Longitude'] = df_store['Longitude'].replace('[,]', '.', regex=True).astype('float')
# Data cleansing for df_transaction by changing the Date format to datetime
df_transaction['Date'] = pd.to_datetime(df_transaction['Date'])

# Group by 'TransactionID' and select the row with the maximum 'Date'
df_transaction = df_transaction.sort_values(by='Date', ascending=False) \
    .groupby('TransactionID', as_index=False).first()

# Merge the DataFrames
df_merge = pd.merge(df_transaction, df_customer, on='CustomerID', how='inner')
df_merge = pd.merge(df_merge, df_product.drop(columns=('Price')), on='ProductID', how='inner')
df_merge = pd.merge(df_merge, df_store, on='StoreID', how='inner')

"""### **clustering**"""

df_merge.head()

# Calculate the correlation matrix
correlation_matrix = df_merge.corr()
correlation_matrix

"""Based on the correlation matrix and considering the goal of clustering similar customers, we can either use Qty or TotalAmount as parameter since it's correlation value is similar. For this model, we use TotalAmount for the parameter."""

df_cluster = df_merge.groupby(['CustomerID']).agg({
    'TransactionID': 'count',
    'TotalAmount': 'sum'
}).reset_index()

df_cluster.head()

data_cluster = df_cluster.drop(columns=['CustomerID'])
data_cluster_normalize = preprocessing.normalize(data_cluster)

data_cluster_normalize

K = range(2, 8)
fits = []
score = []

for k in K:
    model = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(data_cluster_normalize)
    fits.append(model)
    score.append(silhouette_score(data_cluster_normalize, model.labels_, metric='euclidean'))

#choose 4 cluster
sns.lineplot(x = K, y = score);

fits[1]

df_cluster['cluster_label'] = fits[1].labels_
df_cluster.groupby(['cluster_label']).agg({
    'CustomerID': 'count',
    'TransactionID': 'mean',  # Remove the space here
    'TotalAmount': 'mean',
})

"""From the result above, it can be conclude that:
1. Customers in Cluster 0 have a relatively high number of transactions (TransactionID) and the highest average total spending (TotalAmount). This cluster represents customers who are frequent shoppers and spend a significant amount of money.

2. Customers in Cluster 1 have the lowest number of transactions (TransactionID)and the lowest  total spending (TotalAmount). These customers are somewhat less frequent shoppers and spend less on average.

3. Customers in Cluster 2 have the highest average number of transactions (TransactionID) and a substantial average total spending (TotalAmount), although it's lower than Cluster 0. This cluster represents customers who are both frequent shoppers and have a relatively high total spending.



"""